{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8a6c61a",
   "metadata": {},
   "source": [
    "# Mutivariate Linear Regression\n",
    "$ H(x) = Wx + b $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d41db43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c91dd972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x21eae9461b0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97aa5b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[73,80,75],\n",
    "                             [93,88,93],\n",
    "                             [89,91,90],\n",
    "                             [96,98,100],\n",
    "                             [73,66,70]])\n",
    "y_train = torch.FloatTensor([[152], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de26e95",
   "metadata": {},
   "source": [
    "## Hypothesis Function\n",
    "$ H(x) = W_{1}x_{1}+ W_{2}x_{2}+W_{3}x_{3} + b $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7992d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.zeros((3,1), requires_grad = True)\n",
    "b = torch.zeros(1, requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55744aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 +  b\n",
    "hypothesis = x_train.matmul(W) +  b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7000d3",
   "metadata": {},
   "source": [
    "## Cost function: MSE\n",
    "### Mean Squared Error (MSE)\n",
    "$cost(W, b) =  \\frac{1}{m} \\sum_{i=1}^{m}(H( x^{i})- y^{i})^2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb9f9f98",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5) must match the size of tensor b (4) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m cost \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean((\u001b[43mhypothesis\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (4) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "cost = torch.mean((hypothesis - y_train) **2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dbfd16",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "Gradient 계산하기\n",
    "- $\\nabla W = \\frac{\\partial cost}{\\partial W} = \\frac{2}{m}\\sum_{i=1}^{m}(Wx^{i}-y^{i})x^{i}$\n",
    "- $ W: = W - \\alpha \\nabla W $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b355d827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr = le-5)\n",
    "\n",
    "# optimizer 사용법\n",
    "optimizer.zero_grad()\n",
    "cost.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "df507b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1/10, hypothesis: tensor([0., 0., 0., 0., 0.]), cost: 29661.800781\n",
      "Epoch    2/10, hypothesis: tensor([67.2578, 80.8397, 79.6523, 86.7394, 61.6605]), cost: 9298.520508\n",
      "Epoch    3/10, hypothesis: tensor([104.9128, 126.0990, 124.2466, 135.3015,  96.1821]), cost: 2915.712402\n",
      "Epoch    4/10, hypothesis: tensor([125.9942, 151.4381, 149.2133, 162.4896, 115.5097]), cost: 915.040527\n",
      "Epoch    5/10, hypothesis: tensor([137.7968, 165.6247, 163.1911, 177.7112, 126.3307]), cost: 287.936005\n",
      "Epoch    6/10, hypothesis: tensor([144.4044, 173.5674, 171.0168, 186.2332, 132.3891]), cost: 91.371010\n",
      "Epoch    7/10, hypothesis: tensor([148.1035, 178.0144, 175.3980, 191.0042, 135.7812]), cost: 29.758139\n",
      "Epoch    8/10, hypothesis: tensor([150.1744, 180.5042, 177.8508, 193.6753, 137.6805]), cost: 10.445305\n",
      "Epoch    9/10, hypothesis: tensor([151.3336, 181.8983, 179.2240, 195.1707, 138.7440]), cost: 4.391228\n",
      "Epoch   10/10, hypothesis: tensor([151.9824, 182.6789, 179.9928, 196.0079, 139.3396]), cost: 2.493135\n"
     ]
    }
   ],
   "source": [
    "# Full Code\n",
    "x_train = torch.FloatTensor([[73,80,75],\n",
    "                             [93,88,93],\n",
    "                             [89,91,90],\n",
    "                             [96,98,100],\n",
    "                             [73,66,70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "W = torch.zeros((3,1), requires_grad = True)\n",
    "b = torch.zeros(1, requires_grad = True)\n",
    "\n",
    "optimizer = optim.SGD([W, b], lr = 1e-5)\n",
    "\n",
    "# batch size\n",
    "nb_epoches = 10\n",
    "\n",
    "for epoch in range(1, nb_epoches + 1):\n",
    "    \n",
    "    hypothesis = x_train.matmul(W) + b\n",
    "    \n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    \n",
    "    optimizer.zero_grad() # gradient 초기화\n",
    "    cost.backward() # gradient 계산\n",
    "    optimizer.step() # 개선\n",
    "           \n",
    "    print('Epoch {:4d}/{}, hypothesis: {}, cost: {:.6f}'.format(\n",
    "        epoch, nb_epoches, hypothesis.squeeze().detach(), cost.item()))\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e266b7",
   "metadata": {},
   "source": [
    "# nn.Module 을 상속해서 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6385b845",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0a932807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MultivariateLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.linear(x)\n",
    "    \n",
    "hypothesis = model(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d83d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F.mse_loss 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d85a1e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "cost = F.mse_loss(prediction, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f498c8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1/20, hypothesis: tensor([12.2422, 16.1643, 15.1508, 16.8191, 12.4312]), cost: 24821.373047\n",
      "Epoch    2/20, hypothesis: tensor([73.7687, 90.1150, 88.0154, 96.1669, 68.8371]), cost: 7780.782715\n",
      "Epoch    3/20, hypothesis: tensor([108.2150, 131.5174, 128.8097, 140.5908, 100.4168]), cost: 2439.462402\n",
      "Epoch    4/20, hypothesis: tensor([127.5001, 154.6971, 151.6488, 165.4621, 118.0972]), cost: 765.240906\n",
      "Epoch    5/20, hypothesis: tensor([138.2970, 167.6747, 164.4356, 179.3865, 127.9960]), cost: 240.461670\n",
      "Epoch    6/20, hypothesis: tensor([144.3417, 174.9404, 171.5944, 187.1823, 133.5380]), cost: 75.971069\n",
      "Epoch    7/20, hypothesis: tensor([147.7258, 179.0083, 175.6023, 191.5468, 136.6409]), cost: 24.411728\n",
      "Epoch    8/20, hypothesis: tensor([149.6203, 181.2859, 177.8462, 193.9903, 138.3782]), cost: 8.250521\n",
      "Epoch    9/20, hypothesis: tensor([150.6809, 182.5611, 179.1024, 195.3583, 139.3510]), cost: 3.184611\n",
      "Epoch   10/20, hypothesis: tensor([151.2746, 183.2751, 179.8057, 196.1242, 139.8957]), cost: 1.596575\n",
      "Epoch   11/20, hypothesis: tensor([151.6068, 183.6749, 180.1994, 196.5529, 140.2008]), cost: 1.098634\n",
      "Epoch   12/20, hypothesis: tensor([151.7927, 183.8988, 180.4198, 196.7929, 140.3717]), cost: 0.942370\n",
      "Epoch   13/20, hypothesis: tensor([151.8967, 184.0242, 180.5431, 196.9272, 140.4675]), cost: 0.893231\n",
      "Epoch   14/20, hypothesis: tensor([151.9548, 184.0946, 180.6122, 197.0024, 140.5212]), cost: 0.877650\n",
      "Epoch   15/20, hypothesis: tensor([151.9872, 184.1340, 180.6508, 197.0445, 140.5514]), cost: 0.872597\n",
      "Epoch   16/20, hypothesis: tensor([152.0053, 184.1562, 180.6724, 197.0680, 140.5684]), cost: 0.870831\n",
      "Epoch   17/20, hypothesis: tensor([152.0153, 184.1686, 180.6844, 197.0811, 140.5781]), cost: 0.870113\n",
      "Epoch   18/20, hypothesis: tensor([152.0208, 184.1757, 180.6911, 197.0884, 140.5836]), cost: 0.869705\n",
      "Epoch   19/20, hypothesis: tensor([152.0237, 184.1797, 180.6949, 197.0925, 140.5868]), cost: 0.869413\n",
      "Epoch   20/20, hypothesis: tensor([152.0253, 184.1821, 180.6969, 197.0947, 140.5887]), cost: 0.869144\n"
     ]
    }
   ],
   "source": [
    "# Full Code\n",
    "x_train = torch.FloatTensor([[73,80,75],\n",
    "                             [93,88,93],\n",
    "                             [89,91,90],\n",
    "                             [96,98,100],\n",
    "                             [73,66,70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "# 모델 초기화\n",
    "model = MultivariateLinearRegressionModel()\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-5)\n",
    "\n",
    "# batch size\n",
    "nb_epoches = 20\n",
    "\n",
    "for epoch in range(1, nb_epoches + 1):\n",
    "    \n",
    "    hypothesis = model(x_train)\n",
    "    \n",
    "    cost = F.mse_loss(hypothesis, y_train)\n",
    "    \n",
    "    optimizer.zero_grad() # gradient 초기화\n",
    "    cost.backward() # gradient 계산\n",
    "    optimizer.step() # 개선\n",
    "           \n",
    "    print('Epoch {:4d}/{}, hypothesis: {}, cost: {:.6f}'.format(\n",
    "        epoch, nb_epoches, hypothesis.squeeze().detach(), cost.item()))\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "72b56651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20, hypothesis: tensor([-21.2776, -30.5031, -27.4767, -30.5932, -23.9000]), cost: 11522.970703\n",
      "Epoch  100/20, hypothesis: tensor([-21.2776, -30.5031, -27.4767, -30.5932, -23.9000]), cost: 0.744663\n",
      "Epoch  200/20, hypothesis: tensor([-21.2776, -30.5031, -27.4767, -30.5932, -23.9000]), cost: 0.737657\n",
      "Epoch  300/20, hypothesis: tensor([-21.2776, -30.5031, -27.4767, -30.5932, -23.9000]), cost: 0.730902\n",
      "Epoch  400/20, hypothesis: tensor([-21.2776, -30.5031, -27.4767, -30.5932, -23.9000]), cost: 0.724386\n",
      "Epoch  500/20, hypothesis: tensor([-21.2776, -30.5031, -27.4767, -30.5932, -23.9000]), cost: 0.718097\n",
      "Epoch  600/20, hypothesis: tensor([-21.2776, -30.5031, -27.4767, -30.5932, -23.9000]), cost: 0.712037\n",
      "Epoch  700/20, hypothesis: tensor([-21.2776, -30.5031, -27.4767, -30.5932, -23.9000]), cost: 0.706164\n",
      "Epoch  800/20, hypothesis: tensor([-21.2776, -30.5031, -27.4767, -30.5932, -23.9000]), cost: 0.700495\n",
      "Epoch  900/20, hypothesis: tensor([-21.2776, -30.5031, -27.4767, -30.5932, -23.9000]), cost: 0.695015\n",
      "Epoch 1000/20, hypothesis: tensor([-21.2776, -30.5031, -27.4767, -30.5932, -23.9000]), cost: 0.689693\n",
      "Epoch 1100/20, hypothesis: tensor([-21.2776, -30.5031, -27.4767, -30.5932, -23.9000]), cost: 0.684550\n",
      "Epoch 1200/20, hypothesis: tensor([-21.2776, -30.5031, -27.4767, -30.5932, -23.9000]), cost: 0.679575\n",
      "Epoch 1300/20, hypothesis: tensor([-21.2776, -30.5031, -27.4767, -30.5932, -23.9000]), cost: 0.674741\n",
      "Epoch 1400/20, hypothesis: tensor([-21.2776, -30.5031, -27.4767, -30.5932, -23.9000]), cost: 0.670049\n",
      "Epoch 1500/20, hypothesis: tensor([-21.2776, -30.5031, -27.4767, -30.5932, -23.9000]), cost: 0.665500\n",
      "Epoch 1600/20, hypothesis: tensor([-21.2776, -30.5031, -27.4767, -30.5932, -23.9000]), cost: 0.661078\n",
      "Epoch 1700/20, hypothesis: tensor([-21.2776, -30.5031, -27.4767, -30.5932, -23.9000]), cost: 0.656774\n",
      "Epoch 1800/20, hypothesis: tensor([-21.2776, -30.5031, -27.4767, -30.5932, -23.9000]), cost: 0.652607\n",
      "Epoch 1900/20, hypothesis: tensor([-21.2776, -30.5031, -27.4767, -30.5932, -23.9000]), cost: 0.648531\n",
      "Epoch 2000/20, hypothesis: tensor([-21.2776, -30.5031, -27.4767, -30.5932, -23.9000]), cost: 0.644569\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#torch.manual_seed(1)\n",
    "# 데이터\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "class MultivariateLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 1) # 다중 선형 회귀이므로 input_dim=3, output_dim=1.\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "model = MultivariateLinearRegressionModel()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5) \n",
    "\n",
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs+1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    # model(x_train)은 model.forward(x_train)와 동일함.\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n",
    "\n",
    "    # cost로 H(x) 개선하는 부분\n",
    "    # gradient를 0으로 초기화\n",
    "    optimizer.zero_grad()\n",
    "    # 비용 함수를 미분하여 gradient 계산\n",
    "    cost.backward()\n",
    "    # W와 b를 업데이트\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "    # 100번마다 로그 출력\n",
    "       print('Epoch {:4d}/{}, hypothesis: {}, cost: {:.6f}'.format(\n",
    "        epoch, nb_epoches, hypothesis.squeeze().detach(), cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7287cde3",
   "metadata": {},
   "source": [
    "# Minibatch Gradient Descient\n",
    "- 업데이트가 좀 더 빠르게 가능\n",
    "- 전체 데이터를 쓰지 않기 때문에, 잘못된 방향으로 갈 수도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "23520b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.x_data = [[73, 80, 75],\n",
    "                       [93, 88, 93],\n",
    "                       [89, 91, 90],\n",
    "                       [96, 98, 100],\n",
    "                       [73, 66, 70]]\n",
    "        self.y_data = [[152], [185], [180], [196], [142]]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.FloatTensor(self.x_data[idx])\n",
    "        y = torch.FloatTensor(self.y_data[idx])\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "dataset = CustomDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "11d2b2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size = 2, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a2a5f165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1/20, Batch 1/3, cost: 1.471966\n",
      "Epoch    1/20, Batch 2/3, cost: 0.016109\n",
      "Epoch    1/20, Batch 3/3, cost: 0.296724\n",
      "Epoch    2/20, Batch 1/3, cost: 0.853835\n",
      "Epoch    2/20, Batch 2/3, cost: 1.008337\n",
      "Epoch    2/20, Batch 3/3, cost: 0.141416\n",
      "Epoch    3/20, Batch 1/3, cost: 0.982021\n",
      "Epoch    3/20, Batch 2/3, cost: 1.211732\n",
      "Epoch    3/20, Batch 3/3, cost: 0.002839\n",
      "Epoch    4/20, Batch 1/3, cost: 0.431198\n",
      "Epoch    4/20, Batch 2/3, cost: 0.327097\n",
      "Epoch    4/20, Batch 3/3, cost: 2.170290\n",
      "Epoch    5/20, Batch 1/3, cost: 0.130226\n",
      "Epoch    5/20, Batch 2/3, cost: 0.672016\n",
      "Epoch    5/20, Batch 3/3, cost: 2.414065\n",
      "Epoch    6/20, Batch 1/3, cost: 0.387675\n",
      "Epoch    6/20, Batch 2/3, cost: 0.421972\n",
      "Epoch    6/20, Batch 3/3, cost: 2.774998\n",
      "Epoch    7/20, Batch 1/3, cost: 0.696806\n",
      "Epoch    7/20, Batch 2/3, cost: 1.165109\n",
      "Epoch    7/20, Batch 3/3, cost: 0.005432\n",
      "Epoch    8/20, Batch 1/3, cost: 1.466588\n",
      "Epoch    8/20, Batch 2/3, cost: 0.012930\n",
      "Epoch    8/20, Batch 3/3, cost: 0.304925\n",
      "Epoch    9/20, Batch 1/3, cost: 0.800179\n",
      "Epoch    9/20, Batch 2/3, cost: 1.473013\n",
      "Epoch    9/20, Batch 3/3, cost: 0.005039\n",
      "Epoch   10/20, Batch 1/3, cost: 0.159045\n",
      "Epoch   10/20, Batch 2/3, cost: 0.656671\n",
      "Epoch   10/20, Batch 3/3, cost: 2.526099\n",
      "Epoch   11/20, Batch 1/3, cost: 1.192028\n",
      "Epoch   11/20, Batch 2/3, cost: 1.087613\n",
      "Epoch   11/20, Batch 3/3, cost: 0.142808\n",
      "Epoch   12/20, Batch 1/3, cost: 1.572990\n",
      "Epoch   12/20, Batch 2/3, cost: 0.094168\n",
      "Epoch   12/20, Batch 3/3, cost: 0.123961\n",
      "Epoch   13/20, Batch 1/3, cost: 0.126646\n",
      "Epoch   13/20, Batch 2/3, cost: 0.729558\n",
      "Epoch   13/20, Batch 3/3, cost: 2.462288\n",
      "Epoch   14/20, Batch 1/3, cost: 1.107707\n",
      "Epoch   14/20, Batch 2/3, cost: 0.884149\n",
      "Epoch   14/20, Batch 3/3, cost: 0.077437\n",
      "Epoch   15/20, Batch 1/3, cost: 0.102179\n",
      "Epoch   15/20, Batch 2/3, cost: 0.774865\n",
      "Epoch   15/20, Batch 3/3, cost: 2.028069\n",
      "Epoch   16/20, Batch 1/3, cost: 0.216789\n",
      "Epoch   16/20, Batch 2/3, cost: 1.706137\n",
      "Epoch   16/20, Batch 3/3, cost: 0.294235\n",
      "Epoch   17/20, Batch 1/3, cost: 0.860016\n",
      "Epoch   17/20, Batch 2/3, cost: 0.004803\n",
      "Epoch   17/20, Batch 3/3, cost: 1.996165\n",
      "Epoch   18/20, Batch 1/3, cost: 1.706116\n",
      "Epoch   18/20, Batch 2/3, cost: 0.127483\n",
      "Epoch   18/20, Batch 3/3, cost: 0.204825\n",
      "Epoch   19/20, Batch 1/3, cost: 1.531678\n",
      "Epoch   19/20, Batch 2/3, cost: 0.054352\n",
      "Epoch   19/20, Batch 3/3, cost: 0.252263\n",
      "Epoch   20/20, Batch 1/3, cost: 0.906383\n",
      "Epoch   20/20, Batch 2/3, cost: 1.401353\n",
      "Epoch   20/20, Batch 3/3, cost: 0.060795\n"
     ]
    }
   ],
   "source": [
    "nb_epoches = 20\n",
    "\n",
    "for epoch in range(1, nb_epoches + 1):\n",
    "    for batch_idx, samples in enumerate(dataloader):\n",
    "        x_train, y_train = samples\n",
    "        \n",
    "        hypothesis = model(x_train)\n",
    "    \n",
    "        cost = F.mse_loss(hypothesis, y_train)\n",
    "    \n",
    "        optimizer.zero_grad() # gradient 초기화\n",
    "        cost.backward() # gradient 계산\n",
    "        optimizer.step() # 개선\n",
    "           \n",
    "        print('Epoch {:4d}/{}, Batch {}/{}, cost: {:.6f}'.format(\n",
    "        epoch, nb_epoches, batch_idx+1, len(dataloader), cost.item()))\n",
    "  \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
