{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1FHED9t3Los"
   },
   "source": [
    "# Linear Regression\n",
    "- Data definition\n",
    "- Hypothesis\n",
    "- Compute loss\n",
    "- Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입출력은 x, y 로 구분\n",
    "x_train = torch.FloatTensor([[1],[2],[3]])\n",
    "y_train = torch.FloatTensor([[2],[4],[6]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n",
    "$y = Wx + b$\n",
    "또는 $H(x) = Wx + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W, b 초기화(0)\n",
    "W = torch.zeros(1, requires_grad = True) # 학습을 명시\n",
    "b = torch.zeros(1, requires_grad = True)\n",
    "hypothesis = x_train * W + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute loss \n",
    "### Mean Squared Error (MSE)\n",
    "$cost(W, b) =  \\frac{1}{m} \\sum_{i=1}^{m}(H( x^{i})- y^{i})^2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = torch.mean((hypothesis - y_train) ** 2) # 평균 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epoches = 1000\n",
    "\n",
    "for epoch in range(1, nb_epoches + 1):\n",
    "    hypothesis = x_train * W + b\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD([W,b], lr=0.01) # torch.optim 라이브러리 사용\n",
    "\n",
    "optimizer.zero_grad() # gradient 초기화\n",
    "cost.backward() # gradient 계산\n",
    "optimizer.step() # 개선"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deeper Loot at GD\n",
    "## Simpler Hypothesis Function\n",
    "$H(x) = Wx$ 로 가정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입출력은 x, y 로 구분\n",
    "x_train = torch.FloatTensor([[1],[2],[3]])\n",
    "y_train = torch.FloatTensor([[1],[2],[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cost function: Intuition\n",
    "W = 1 일 때, cost = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = torch.mean((hypothesis - y_train) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent : Intuition\n",
    "Gradient 계산하기\n",
    "- $\\frac{\\partial cost}{\\partial W} = \\nabla W$\n",
    "- $cost(W) = \\frac{1}{m}\\sum_{i=1}^{m}(Wx^{i}-y^{i})^{2}$\n",
    "- $\\nabla W = \\frac{\\partial cost}{\\partial W} = \\frac{2}{m}\\sum_{i=1}^{m}(Wx^{i}-y^{i})x^{i}$\n",
    "- $ W: = W - \\alpha \\nabla W $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "a leaf Variable that requires grad is being used in an in-place operation.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m gradient \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean((W \u001b[38;5;241m*\u001b[39m x_train \u001b[38;5;241m-\u001b[39m y_train) \u001b[38;5;241m*\u001b[39m x_train)\n\u001b[0;32m      2\u001b[0m lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m----> 3\u001b[0m W \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m lr \u001b[38;5;241m*\u001b[39m gradient\n",
      "\u001b[1;31mRuntimeError\u001b[0m: a leaf Variable that requires grad is being used in an in-place operation."
     ]
    }
   ],
   "source": [
    "gradient = 2 * torch.mean((W * x_train - y_train) * x_train)\n",
    "lr = 0.1\n",
    "W -= lr * gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1/10, W: 0.000, cost: 4.666667\n",
      "Epoch    2/10, W: 1.400, cost: 0.746666\n",
      "Epoch    3/10, W: 0.840, cost: 0.119467\n",
      "Epoch    4/10, W: 1.064, cost: 0.019115\n",
      "Epoch    5/10, W: 0.974, cost: 0.003058\n",
      "Epoch    6/10, W: 1.010, cost: 0.000489\n",
      "Epoch    7/10, W: 0.996, cost: 0.000078\n",
      "Epoch    8/10, W: 1.002, cost: 0.000013\n",
      "Epoch    9/10, W: 0.999, cost: 0.000002\n",
      "Epoch   10/10, W: 1.000, cost: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# model 초기화\n",
    "W = torch.zeros(1)\n",
    "# 학습률 설정\n",
    "lr = 0.1\n",
    "\n",
    "nb_epoches = 10\n",
    "\n",
    "for epoch in range(1, nb_epoches + 1):\n",
    "    \n",
    "    hypothesis = x_train * W \n",
    "    \n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    gradient = torch.sum((W * x_train - y_train) * x_train)\n",
    "    \n",
    "    print('Epoch {:4d}/{}, W: {:.3f}, cost: {:.6f}'.format(\n",
    "        epoch, nb_epoches, W.item(), cost.item()))\n",
    "\n",
    "    # cost gradient 로 H(x) 개선\n",
    "    W -= lr * gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1/10, W: 0.000, cost: 4.666667\n",
      "Epoch    2/10, W: 1.400, cost: 0.746667\n",
      "Epoch    3/10, W: 0.840, cost: 0.119467\n",
      "Epoch    4/10, W: 1.064, cost: 0.019115\n",
      "Epoch    5/10, W: 0.974, cost: 0.003058\n",
      "Epoch    6/10, W: 1.010, cost: 0.000489\n",
      "Epoch    7/10, W: 0.996, cost: 0.000078\n",
      "Epoch    8/10, W: 1.002, cost: 0.000013\n",
      "Epoch    9/10, W: 0.999, cost: 0.000002\n",
      "Epoch   10/10, W: 1.000, cost: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# 모델 초기화\n",
    "W = torch.zeros(1, requires_grad = True)\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W], lr=0.15) \n",
    "\n",
    "# batch size\n",
    "nb_epoches = 10\n",
    "\n",
    "for epoch in range(1, nb_epoches + 1):\n",
    "    \n",
    "    hypothesis = x_train * W \n",
    "    \n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    gradient = torch.sum((W * x_train - y_train) * x_train)\n",
    "    \n",
    "    print('Epoch {:4d}/{}, W: {:.3f}, cost: {:.6f}'.format(\n",
    "        epoch, nb_epoches, W.item(), cost.item()))\n",
    "  \n",
    "    optimizer.zero_grad() # gradient 초기화\n",
    "    cost.backward() # gradient 계산\n",
    "    optimizer.step() # 개선\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
